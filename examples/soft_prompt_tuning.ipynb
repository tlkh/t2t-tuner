{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f152682d",
   "metadata": {},
   "source": [
    "## Soft Prompt Tuning\n",
    "\n",
    "Soft Prompt Tuning on Sentiment Analysis Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a055e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import t2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a534cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-364cfa96496637f3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to ../cache/json/default-364cfa96496637f3/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550e0f6f40a2411fbde9e75372d2c7e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e84456866346ff96db27bf08f01ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9aa129a4eeb4e6ea4c716c306d3fa74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc300b35e9be4832b266a37a6e2907b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to ../cache/json/default-364cfa96496637f3/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dca0cc7c415428caab4c1af517250b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3325f694b6dd4768b5bb487c40853ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e943e9c1c3645c99d22d8567250abe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39cb6afe3f5450c831c41a274253eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86b0c0f247f41faa4545ce09f025cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_arguments = t2t.TrainerArguments(\n",
    "    model_name_or_path=\"t5-large\",\n",
    "    output_dir=\"/tmp/saved_model\",\n",
    "    cache_dir=\"/workspace/cache\",\n",
    "    overwrite_output_dir=True,\n",
    "    train_file=\"../sample_data/trainlines.json\",\n",
    "    validation_file=\"../sample_data/validlines.json\",\n",
    "    source_id=\"s\",\n",
    "    target_id=\"t\",\n",
    "    max_source_length=128,\n",
    "    max_target_length=8,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    learning_rate=0.1,\n",
    "    fp16=False,\n",
    ")\n",
    "trainer = t2t.Trainer(arguments=trainer_arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac07ee",
   "metadata": {},
   "source": [
    "### Prompt Tuning Conversion\n",
    "\n",
    "* Add in new tokens into tokenizer and model embedding layer\n",
    "* Freeze entire model except for embedding parameters for the new tokens\n",
    "* Initialize the new tokens (currently to the mean of the embedding layer)\n",
    "\n",
    "In this example, we use 5 prompt tokens, which has been demonstrated to be effective.\n",
    "\n",
    "Note: entire embedding layer is marked as trainable, but the gradients are zeroed out for all the embeddings except the newly added prompt tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed35ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      "=======\n",
      "- model name: t5-large\n",
      "- model params:\n",
      "  - train: 32.9 M\n",
      "  - total: 737.7 M\n",
      "  - vocab: 32105\n",
      "- prompt tuning only: True\n"
     ]
    }
   ],
   "source": [
    "prompt_list = [\"<p1>\",\"<p2>\",\"<p3>\",\"<p4>\",\"<p5>\"]\n",
    "trainer.convert_to_prompt_tuning(prompt_list=prompt_list)\n",
    "trainer.model_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e87a07",
   "metadata": {},
   "source": [
    "Examples of tokenization and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e45ae6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: <p1><p2><p3><p4><p5> XXX\n",
      "tokenized: [32100, 32101, 32102, 32103, 32104, 3, 4, 4, 4, 1]\n",
      "prompt tokens -> [32100, 32101, 32102, 32103, 32104]\n",
      "example of frozen token:\n",
      "tensor([ 0.7617, -1.4844,  3.0625,  1.8516, -1.7812, -5.5938,  0.7852,  2.3594,\n",
      "        -2.0781, -6.2188])\n",
      "^ this should not change after training\n",
      "\n",
      "example of prompt token:\n",
      "tensor([-4.3131, -2.5815,  2.5195,  2.8988, -2.4462, -3.0837, -2.9662, -3.0946,\n",
      "         3.9238, -2.4886])\n",
      "^ this is trainable\n"
     ]
    }
   ],
   "source": [
    "test_seq = \"<p1><p2><p3><p4><p5> XXX\"\n",
    "print(\"input:\", test_seq)\n",
    "tokenized_seq = trainer.tokenizer(test_seq)[\"input_ids\"]\n",
    "print(\"tokenized:\", tokenized_seq)\n",
    "print(\"prompt tokens ->\", tokenized_seq[:5])\n",
    "\n",
    "for name, param in trainer.model.shared.named_parameters():\n",
    "    print(\"example of frozen token:\")\n",
    "    frozen_token_example = param.data[tokenized_seq[0]-1][:10]\n",
    "    print(frozen_token_example)\n",
    "    print(\"^ this should not change after training\\n\")\n",
    "    print(\"example of prompt token:\")\n",
    "    print(param.data[tokenized_seq[0]][:10])\n",
    "    print(\"^ this is trainable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed10431",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a640bad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9487baea60e4ae0b6cc27ac4bc01348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b36f8170b3c4c0a9e448341bf68ce07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 8000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 750\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 07:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>15.700900</td>\n",
       "      <td>0.314461</td>\n",
       "      <td>2.274800</td>\n",
       "      <td>2.043500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.419400</td>\n",
       "      <td>0.222372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.342800</td>\n",
       "      <td>0.211115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2001\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /tmp/saved_model/checkpoint-250\n",
      "Configuration saved in /tmp/saved_model/checkpoint-250/config.json\n",
      "Model weights saved in /tmp/saved_model/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /tmp/saved_model/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/saved_model/checkpoint-250/special_tokens_map.json\n",
      "Copy vocab file to /tmp/saved_model/checkpoint-250/spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2001\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /tmp/saved_model/checkpoint-500\n",
      "Configuration saved in /tmp/saved_model/checkpoint-500/config.json\n",
      "Model weights saved in /tmp/saved_model/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /tmp/saved_model/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/saved_model/checkpoint-500/special_tokens_map.json\n",
      "Copy vocab file to /tmp/saved_model/checkpoint-500/spiece.model\n",
      "Deleting older checkpoint [/tmp/saved_model/checkpoint-250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2001\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /tmp/saved_model/checkpoint-750\n",
      "Configuration saved in /tmp/saved_model/checkpoint-750/config.json\n",
      "Model weights saved in /tmp/saved_model/checkpoint-750/pytorch_model.bin\n",
      "tokenizer config file saved in /tmp/saved_model/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/saved_model/checkpoint-750/special_tokens_map.json\n",
      "Copy vocab file to /tmp/saved_model/checkpoint-750/spiece.model\n",
      "Deleting older checkpoint [/tmp/saved_model/checkpoint-500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to /tmp/saved_model\n",
      "Configuration saved in /tmp/saved_model/config.json\n",
      "Model weights saved in /tmp/saved_model/pytorch_model.bin\n",
      "tokenizer config file saved in /tmp/saved_model/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/saved_model/special_tokens_map.json\n",
      "Copy vocab file to /tmp/saved_model/spiece.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               = 12287178GF\n",
      "  train_loss               =     5.4877\n",
      "  train_runtime            = 0:07:15.95\n",
      "  train_samples            =       8000\n",
      "  train_samples_per_second =     55.052\n",
      "  train_steps_per_second   =       1.72\n"
     ]
    }
   ],
   "source": [
    "trainer.train(valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef61f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original frozen token: tensor([ 0.7617, -1.4844,  3.0625,  1.8516, -1.7812, -5.5938,  0.7852,  2.3594,\n",
      "        -2.0781, -6.2188])\n",
      "current frozen token: tensor([ 0.7617, -1.4844,  3.0625,  1.8516, -1.7812, -5.5938,  0.7852,  2.3594,\n",
      "        -2.0781, -6.2188], device='cuda:0')\n",
      "^ this should not have changed\n",
      "\n",
      "prompt token:\n",
      "tensor([ 1.3241, -2.8448,  2.0462,  ..., -0.2151,  8.1019, 13.3674],\n",
      "       device='cuda:0')\n",
      "^ this is trainable, so should have changed\n"
     ]
    }
   ],
   "source": [
    "for name, param in trainer.model.shared.named_parameters():\n",
    "    print(\"original frozen token:\", frozen_token_example)\n",
    "    print(\"current frozen token:\", param.data[tokenized_seq[0]-1][:10])\n",
    "    print(\"^ this should not have changed\\n\")\n",
    "    print(\"prompt token:\")\n",
    "    print(param.data[tokenized_seq[0]])\n",
    "    print(\"^ this is trainable, so should have changed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aed457",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b08706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"<p1><p2><p3><p4><p5> This is the worst movie I have ever seen!\"\n",
    "trainer.generate_single(input_text, max_length=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e470e57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"<p1><p2><p3><p4><p5> This is the best movie I have ever seen!\"\n",
    "trainer.generate_single(input_text, max_length=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7229f94d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
